%% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{array}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{plain}
\newtheorem{prop}[thm]{\protect\propositionname}
\theoremstyle{definition}
\newtheorem{example}[thm]{\protect\examplename}

\makeatother

\usepackage{babel}
\providecommand{\examplename}{Example}
\providecommand{\propositionname}{Proposition}
\providecommand{\theoremname}{Theorem}

\begin{document}
\global\long\def\G{\mathcal{G}}%
\global\long\def\F{\mathcal{F}}%
\global\long\def\var{\mathcal{\mathrm{var}}}%
\global\long\def\E{\mathbb{E}}%
\global\long\def\defined{\stackrel{\text{def}}{=}}%
\global\long\def\Bi{\mathcal{\mathrm{Bi}}}%
\global\long\def\indep#1{{\perp\hspace{-2mm}\perp}#1}%
\global\long\def\P{\mathbb{P}}%
\global\long\def\N{\mathbb{N}}%
\global\long\def\U{\mathbb{U}}%
\global\long\def\E{\mathbb{E}}%
\global\long\def\R{\mathbb{R}}%
\global\long\def\G{\mathcal{G}}%
\global\long\def\g{\mathbb{\Pi}}%
\global\long\def\F{\mathcal{F}}%
\global\long\def\S{\mathcal{S}}%
\global\long\def\Q{\mathcal{Q}}%
\global\long\def\B{\mathcal{B}}%
\global\long\def\ND{\mathcal{N}}%
\global\long\def\XX{\mathcal{X}}%
\global\long\def\indep#1{{\perp\hspace{-2mm}\perp}#1}%
\global\long\def\L{\mathcal{L}}%
\global\long\def\var{\mathrm{var}}%
\global\long\def\cov{\mathrm{cov}}%
\global\long\def\charf{\mathbf{1}}%
\global\long\def\d{\mathrm{d}}%
\global\long\def\M{\mathcal{M}}%
\global\long\def\T{\mathcal{T}}%
\global\long\def\Exp{\mathrm{Exp}}%
\global\long\def\Uniform{\mathrm{U}}%
\global\long\def\eqd{{d\atop =}}%
\global\long\def\A{\mathcal{A}}%
\global\long\def\I{\mathcal{I}}%
\global\long\def\X{\mathcal{X}}%
\global\long\def\supp{\mathrm{support}}%
\global\long\def\H{\mathcal{H}}%
\global\long\def\Z{\mathcal{Z}}%
\global\long\def\as{\qquad a.s.}%
\global\long\def\on{\qquad\text{on }}%
\global\long\def\C{\mathcal{C}}%
\global\long\def\barxi{\overline{\xi}}%
\global\long\def\Po{\mathrm{Po}}%
\global\long\def\barvs{\overline{\varsigma}}%
\global\long\def\bareps{\overline{\varepsilon}}%
\global\long\def\bari{\overline{\iota}}%
\global\long\def\barx{\overline{x}}%
\global\long\def\baru{\overline{u}}%
\global\long\def\bars{\overline{s}}%
\global\long\def\Bi{\mathrm{Bi}}%
\global\long\def\defined{\stackrel{\text{def }}{=}}%
\global\long\def\d{\mathbf{d}}%
\global\long\def\dw{\mathrm{d}}%
\global\long\def\bary{\overline{y}}%
\global\long\def\cvar{\mathrm{CVaR}}%
\global\long\def\barP{P}%
\global\long\def\barX{\overline{X}}%
\global\long\def\barP{\overline{P}}%
\global\long\def\barT{\overline{T}}%
\global\long\def\barB{\overline{B}}%
\global\long\def\barI{\overline{I}}%

\title{SEIR Filter -- Stochastic Model of Pandemics}
\maketitle

\section{Model Definition}

Assume an infinite population, each individual of which finds himself
in one of the compartments $S_{0},\dots,S_{k}$. Let $I_{t}\in\N_{0}^{k},$
$t\in\N_{0}^{+},$ be an external inflow (import) of individuals into
the compartments $(1,\dots,k)$ and let $Z_{t}\in\R^{n},$ $t\in\N_{0}^{+}$,
be an observed exogenous process.

For any $t\in\N_{0}$, let $X_{t}=(X_{t}^{1},\dots,X_{t}^{k})\in\N_{0}^{k},t\in\N_{0},$
be a possibly hidden stochastic process of the compartment sizes which
we define later. Let 
\[
Y_{t}\in\R^{n},\qquad Y_{t}=FX_{t}+\epsilon_{t},\qquad t\in\N_{0},
\]
be a process of observations where $F$ is a deterministic $n\times k$
matrix with rank $n$, and $\epsilon_{t}$ is a random vector. Denote
$\F_{t}$ and $\G_{t}$ the filtrations induced by $(X,Y,I,Z)$, by
$(Y,I,Z)$, respectively, $\F_{t}$ representing all the information,
$\G_{t}$ the observable one. We assume that 
\[
\E(\epsilon_{t+1}|\F_{t})=0,\qquad\var(\epsilon_{t+1}|\F_{t})=\mathrm{diag(}\Gamma_{t}X_{t}),\qquad t\in\N_{0},
\]
where $\Gamma_{t}\in\G_{t}$ is a random matrix.

We define $X$ recursively. Namely, we let $X_{0}$ be possibly random
and, for any $t\in\N$, we put 
\[
X_{t+1}=I_{t}+M_{t+1}+N_{t+1}.
\]
Here, $N_{t+1}\in\N_{0}^{k}$ is the inflow (of the newly infected)
from $S_{0}$ into the remaining compartments fulfilling $N_{t+1}|\F_{t}\text{\ensuremath{\sim\Po(B_{t}X_{t})}}$
where $B_{t}=(\beta_{t}^{ij})_{1\leq i,j\leq k}\in\G_{t}$ is a random
matrix and, for any vector $x$, $\Po(x)$ stands for a vector of
independent Poisson variables with the intensities given by $x$.
Further, $M_{t+1}\in\N_{0}$ is the vector such that, for each $i$,
with $M_{t+1}^{i}$is the number of the individuals who found themselves
in one of the compartments $(1,\dots,k)$ at $t$ and are in compartment
$i$ at $t+1.$ Assuming the individuals to change their state according
to a common transition matrix $P_{t}=(p_{t}^{ij})_{1\leq i,j\leq k}\in\G_{t}$
with the changes being conditionally independent given $\F_{t},$
we get $M_{t+1}|\F_{t}\sim\bigcirc_{1\leq i\leq k}\mathrm{Mult}(X_{t}^{i},P_{t}^{i})$,
where $\bigcirc$ stands for convolution, $\mathrm{Mult}$ stands
for the multinomial distribution and, for any matrix $A$, $A^{i}$
is the $i$-th column of $A$. 

Finally, we assume that $N_{t+1},M_{t+1}$ and $\epsilon_{t+1}$ are
mutually conditionally independent given $\F_{t}$ (which roughly
means that all the dependence between the inflows, the transitions
and the observation can be explained by the state of the system at
$t$).

\section{Model Properties}

By probability calculus, we get that 
\begin{equation}
\E\left[\left.{X_{t+1}\atop Y_{t+1}}\right|\F_{t}\right]=\left[{E\atop F}\right](T_{t}X_{t}+I_{t}),\label{eq:moments}
\end{equation}
\[
\var\left(\left.{X_{t+1}\atop Y_{t+1}}\right|\F_{t}\right)=\left[{E\atop F}\right]\Lambda_{t}(X_{t})\left[{E\atop F}\right]^{T}+\mathrm{diag\left({0_{k}\atop \Gamma_{t}X_{t}}\right),\qquad}t\geq0,
\]
where $E$ is the identity matrix and
\[
T_{t}\defined P_{t}+B_{t},\qquad\Lambda_{t}(X_{t})\defined\sum_{i=1}^{k}\mathrm{[diag}(P_{t}^{i})-P_{t}^{i}(P_{t}^{i}){}^{T}]X_{t}^{i}+\mathrm{diag}(B_{t}X_{t}),\qquad t\geq0.
\]
Note that 
\[
\Lambda_{t}(x)=\sum_{i=1}^{k}\Phi_{t,i}x^{i},\qquad\Phi_{t,i}\defined\mathrm{diag}(B_{t}^{i}+P_{t}^{i})-P_{t}^{i}(P_{t}^{i})^{T},\qquad t\geq0,x\in\R_{+}^{k},
\]
i.e. $\Lambda_{t}$ is linear in $x$.

Consequently, for any $t,s\in\N_{0},s>t$, 
\begin{multline*}
\E(X_{t}|\F_{s})=\E(T_{s,t-1}X_{s}+\sum_{\theta=s}^{t-1}T_{\theta+1,t-1}I_{\theta}|\F_{s})=\E(T_{s,t-1}|\F_{s})X_{s}+\sum_{\theta=s}^{t-1}\E(T_{\theta+1,t-1}I_{\theta}|\F_{s}),
\end{multline*}
In the special case that 
\begin{equation}
B_{\tau}\equiv B_{s},\qquad P_{\tau}\equiv P_{s},\qquad s\leq\tau\leq t,\label{eq:speck}
\end{equation}
we have 
\[
\E(X_{t}|\F_{s})=T_{s}^{t-s}X_{s}+\sum_{\tau=s}^{t-1}T_{s}^{t-\tau-1}\E(I_{\tau}|\F_{s}),
\]
and 
\[
\E(X_{t}|\G_{s})=T_{s}^{t-s}\E(X_{s}|\G_{s})+\sum_{\tau=s}^{t-1}T_{s}^{t-\tau-1}\E(I_{\theta}|\G_{s})
\]
where, for any matrix process $A_{t}$, $A_{t,s}\defined\prod_{\theta=t}^{s}A_{\theta}$
with $A_{t,t-1}\defined E$. If, in addition, $\E(I_{\theta}|\G_{s})\equiv\mu$
for some $\mu\in\G_{s}$ and $(E-T_{s})$ is invertible, then the
formula further simplifies
\[
\E(X_{t}|\G_{s})=T_{s}^{t-s}\E(X_{s}|\G_{s})+(E-T_{s})^{-1}(E-T_{s}^{t-s})\mu.
\]


\section{Sub-models and Replication Number}

We say that the subset of states $D=\{s_{1},\dots,s_{m}\}$ is $subepidemic$
if, for any $t$ and any $1\leq i\leq m$, both the $s_{i}$-th row
and the $s_{i}$-th column of $B_{t}$ are identically zero and $s_{i}$
is not accessible outside $D$ in the matrix $P_{t}.$ In words, this
means that, for any $i\in D$ , the $i$-th compartment does not increase
through direct infection, the infection does not depend on the compartment
size and it is impossible to get to the state $i$ once being outside
$D$. 

Let, after a possible re-ordering, $m\in\N$ be such that $\{1,\dots,m\}$
is subepidemic (such $m$ always exists because it can be always put
to $k$). For any vector $x\in\R^{k}$, denote $\overline{x}$ its
restriction to $(1,\dots,m)$ and, for any matrix $A\in\R^{k\times k}$,
denote $\overline{A}$ its restriction to $(1,\dots,m)\times(1,\dots,m)$

Observe that $\barX$ follows its own version of our model, namely
that 
\[
\left.\barX_{t+1}\right|\F_{t}=\bigcirc_{1\leq i\leq m}\mathrm{Mult}(\barX_{t}^{i},\barP_{t}^{i})\circ\Po(\barB_{t}\barX_{t})\circ\delta(\barI_{t}),
\]
The only difference is that, $\barP_{t}$ is not necessarily a transition
matrix (i.e. the sums of their columns may be less than one).

For any $t$, we define the reproduction number $r_{t}$ (of a self-sufficient
set $\{1,\dots,m\}$) as 
\[
r_{t}\defined\sum_{\tau=t}^{\infty}\mathbf{1}^{T}\E(B_{\tau}\barP_{t,\tau-1}\pi_{t}|\F_{t-1}),\qquad\pi_{t}=\E\left\{ \left.\nu\left(\overline{N}_{t}+\overline{I}_{t-1}\right)\right|\F_{t-1}\right\} .
\]
Observe that $r_{t}$ complies with the usual definition of reproduction
number as it equals to the conditional expectation (w.r.t. $\F_{t-1})$
of the infections caused by an individual having arrived at $t$.
To see it, note that $\pi_{t}$ is the conditional distribution of
the state in which a randomly chosen newcomer (the one brought by
the import or by the infection) finds himself at $t$, and observe
that, for each newcomer at $t$, the expected number of those infected
by him at $t+1$ is given by the sum of the components of $_{t}\pi_{t}$,
the expected number infected at $t+1$ is given by the sum of components
of $B_{t+1}\barP_{t}\pi_{t}$ etc. 

If $\barX$ is not observed, then the reproduction number has to be
estimated, most naturally by its conditional expectation with respect
to the known information: 
\[
\tilde{r}_{t}\defined\sum_{\tau=t}^{\infty}\mathbf{1}^{T}\E(\barB_{\tau}\barP_{t,\tau-1}\pi_{t}|\G_{t-1}).
\]
In the special case of $\barB_{\tau}\equiv\barB_{t-1},\barP_{\tau}\equiv\barP_{t-1}$,
$\tau\geq t$, with $\rho(\barP_{t-1})<1$ where $\rho$ is the spectral
radius, the formula simplifies to
\[
\tilde{r}_{t}=\mathbf{1}^{T}\barB_{t-1}\left(\sum_{i=0}^{\infty}\barP_{t-1}^{i}\right)\E(\pi_{t}|\G_{t-1})=\mathbf{1}^{T}\barB_{t-1}(E-\barP_{t-1})^{-1}\E(\pi_{t}|\G_{t-1})
\]
Note and that there could be difficulties computing $\pi_{t}$ --
yet the estimate $\pi_{t}\doteq\nu(\barB_{t-1}\E(\barX_{t-1}|\G_{t-1})+\barI_{t-1})$
seems a straightforward choice, it is generally not unbiased due to
the normalization. This problem, however, vanishes if the imports
and new infections all fall into a single state (typically called
exposed), in which case $\pi_{t}=(1,0,\dots,0)^{T}$. 

\section{Asymptotic behavior}

Keep assuming that $\{1,\dots,m\}$ is subepidemic. The next Proposition
states conditions, for vanishing, explosion and ``stationary'' behavior
of the self-sufficient sub-compartments.
\begin{prop}
\label{prop:as}(i) If $\barT_{t}\leq S$ component-wise, where S
is deterministic with $\sigma\defined\rho(S)<1,$ and if $\E\barI_{t}=o(t^{-\alpha})$
for some $\alpha>0$ then $\barX_{t}\rightarrow0$ almost sure. Here,
$\rho$ denotes the spectral radius of a matrix.\\
(ii) If $\barT_{t}\geq R$ where R is deterministic irreducible with
$\varrho\defined\rho(R)>1$ and either $\E\barX_{0}\neq0$ or $\E\barI_{\tau}\neq0$
for some $\tau$, then $\|\E\barX_{t}\|\rightarrow\infty$.\\
(iii) If $\E\barI_{t}\equiv\mu$ for some $\text{\ensuremath{\mu} }$and
$R\leq\barT_{t}\leq S$ such that $\sigma\defined\rho(S)<1,$ then
\[
\liminf_{t}\E\barI_{t}\ge(E-R)^{-1}\mu,\qquad\limsup_{t}\E\barI_{t}\leq(E-S)^{-1}\mu
\]
\end{prop}

\begin{proof}
(i) We have 
\begin{multline*}
\E\barX_{t}=\E(\E(\barX_{t}|\G_{0}))=\E(\barT_{0,\tau-1}X_{0}+\sum_{\theta=0}^{t-1}\barT_{\theta+1,t-1}\E(\barI_{\theta}|\G_{0}))=\\
=\barT_{0,t-1}\E\barX_{0}+\sum_{\theta=0}^{t-1}\barT_{\theta+1,t-1}\E\barI_{\theta}\leq a_{t}+b_{t,}\qquad a_{t}=S^{t}\E\barX_{0},\qquad b_{t}=\sum_{\theta=0}^{t-1}S^{t-\theta-1}\E\barI_{\theta}
\end{multline*}
Thanks to the sub-unit spectral radius of $S$, $a_{t}\rightarrow0$.
Further, by the non-negativity of $H$ and the properties of convergence,
there exists $c\in\R_{+}^{m}$ such that $\E\barI_{t}\leq c(t+1)^{-1}.$
Thus, for any $\varsigma$ fulfilling $\sigma<\varsigma<1$, we get,
after re-indexing the sum, 
\[
b_{t}=\sum_{\tau=0}^{t-1}S^{\tau}\E\barI_{t-\tau-1}\leq\sum_{\tau=0}^{t-1}S^{\tau}c\frac{1}{(t-\tau)^{\alpha}}=\underbrace{\frac{1}{t^{\alpha}}}_{\rightarrow0}\times\underbrace{\sum_{\tau=0}^{t-1}(\varsigma^{-1}S)^{\tau}c}_{\rightarrow(E-\varsigma^{-1}S)^{-1}c}\underbrace{\left(\frac{\varsigma^{\tau/\alpha}}{t-\tau}\right)^{\alpha}}_{\leq d}\rightarrow0.
\]
Indeed, $\rho(\varsigma^{-1}S)=\frac{\sigma}{\varsigma_{t}}<1$, and
the upper bound $d$ exists as $f(\tau)\defined\frac{t\varsigma^{\tau/\alpha}}{t-\tau}$
increases in $\tau=t-1$ and its derivative has only a single root,
so we have $f(\tau)\leq\max(f(0),f(t-1))=\max(1,\varsigma^{\frac{t-1}{\alpha}}t)\leq d\defined\max(1,\frac{1}{e\varsigma^{1/\alpha}|\alpha^{-1}\ln\varsigma|})$
on $[0,t-1].$ Finally, thanks to the non-negativity of $\barX,$
convergence of $\E\barX_{t}$ suffices for a.s. convergence of $\barX_{t}$.

(ii) Let $\E\barX_{0}\neq0$ and $\varrho>1$. As $R$ is irreducible
non-negative $\varrho$ is its eigenvalue and the corresponding eigenvector
$x$ is positive by the Perron-Frobenius Theorem. Further, by the
irreducibility of $T$, there exists $n$ such that $y\defined R^{n}\E\barX_{0}>0$
component-wise, so there exist $e>0$ such that $y\geq ex$. Thus
\[
\E\barX_{t}\geq R^{t}\E\barX_{0}\geq R^{t-n}y\geq eR^{t-n}x
\]
norm of which converges to infinity. The proof for $\E\barI_{\tau}\neq0$
is analogous.

(iii) 

$\E\barX_{t}=\sum_{\tau=0}^{t-1}\barT_{\tau,t-2}\mu+\barT_{0,t-1}\E\barX_{0}\leq\sum_{\tau=0}^{t-1}S^{\tau}\mu+S^{t-1}\E\barX_{0}\rightarrow(\sum_{\tau=0}^{\infty}S^{\tau})\mu=(E-S)^{-1}\mu$
and similarly for $R.$
\end{proof}
\begin{example}
\label{exa:e1}Say there are five states $E$ -- exposed, $I_{a}$
-- infectious asymptomatic, who will never exhibit symptoms, $I_{p}$
-- infectious pre-symptomatic, who will later exhibit symptoms, $I_{s}$
-- infectious symptomatic, and $R$ -- removed, including recovered,
dead, and infectious isolated. All the $I_{\bullet}$ states are equally
infectious, i.e. $\beta_{t}^{Ex}=\beta_{t}$, $x\in\{I_{a},I_{p},I_{s}\},$
where $\beta$ is a $\G_{t}$-adapted process. The probability that
the exposed transits to $\{I_{a},I_{p}\}$ is $\sigma,$ the probability
of completely asymptomatic course is $\alpha$, the probability of
transition from $I_{p}$ to $I_{s}$ is $\varsigma$. Further, the
probability of ending $I_{a}$ and $I_{s}$, by natural causes (recovery,
end of infectiousness, death in case of $I_{s}$) is $\varrho_{a}$,
$\varrho_{s}$, respectively. Finally, the probability that a symptomatic
individual isolates herself is $\eta$ and the probability that the
individual is isolated regardless of his state is $\theta_{t}$ for
some $\G_{t}$-adapted process $\theta$. The situation is illustrated
on the following Figure

\includegraphics[scale=0.3]{simple}

If we neglect (small) joint probabilities of natural exits from the
infectious states and the isolations, we get, 
\[
P_{t}=\left[\begin{array}{ccccc}
1-\sigma-\theta_{t} & 0 & 0 & 0 & 0\\
\alpha\sigma & 1-\varrho_{a}-\theta_{t} & 0 & 0 & 0\\
(1-\alpha)\sigma & 0 & 1-\varsigma-\theta_{t} & 0 & 0\\
0 & 0 & \varsigma & 1-\varrho_{s}-\eta-\theta_{t} & 0\\
\theta_{t} & \theta_{t}+\varrho_{a} & \theta_{t} & \theta_{t}+\eta+\varrho_{s} & 1
\end{array}\right],\qquad B_{t}=\left[\begin{array}{ccccc}
0 & \beta_{t} & \beta_{t} & \beta_{t} & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0
\end{array}\right]
\]
Clearly, we can put $m=4$ (the first four states are self-sufficient),
getting 
\[
\barT_{t}=C(\gamma_{t})-\theta_{t}E,\qquad C(\beta)=\left[\begin{array}{cccc}
1-\sigma & \beta & \beta & \beta\\
\alpha\sigma & 1-\varrho_{a} & 0 & 0\\
(1-\alpha)\sigma & 0 & 1-\varsigma & 0\\
0 & 0 & \varsigma & 1-\varrho_{s}-\eta
\end{array}\right].
\]
By the well known rule, we have $\rho(\barT_{t})=\rho(C(\gamma_{t}))-\theta_{t}$,
$g(\gamma)=\rho(C(\gamma))$. So, given that $\rho(\barT_{t})\geq1$,
there are two ways of decreasing the spectral radius: to decrease
the infection rate $\beta_{t}$ (typically by some counter-epidemic
measures) or to increase the isolation rate $\theta_{t}$ (e.g. by
strengthening the tracing capacity). 

Further, once there is a ``target'' spectral radius $\rho_{0},$
all the combinations of $\beta$ and $\theta$ yielding $\rho(\barT_{t})=\rho_{0}$
fulfill $\rho_{0}+\theta-g(\beta)=0$ giving a ``marginal rate of
substitiution'' $\theta(\beta)'=-g'(\beta)$ of the measures by the
quarantine, i.e. how much we have to increase the isolation speed
when releasing the restrictions. 
\end{example}

\begin{example}
Assume the fraction $\nu$ of the population is non-compliant, which
means that, once a restriction on social contacts is imposed, the
non-compliant part applies it only partially. Assume that, without
restrictions, the population is mixed which means that each individual,
compliant or not, has, up to a constant, $(1-\nu)$ contacts with
the compliant individuals and $\nu$ contacts with the non-compliant
ones. Given a restriction, the compliant individuals restrict their
opportunities to contacts by $\phi$ while the non-compliant ones
only to $f(\phi)>\phi$ . As a result, the compliant ones will have,
up to a constant, $\phi^{2}(1-\nu)$ contacts with the compliant ones,
$\phi f(\phi)\nu$ contacts with the non-compliant ones, while the
non-compliant will have $\phi f(\phi)(1-\nu)$ and $f(\phi)^{2}\nu$
contacts with the compliant, non-compliant, respectively. 

Assuming a simple model with compartments $I_{c}$ - infected compliant,
$I_{n}$ - infected non-compliant, and $R$ - removed, where the course
of infection is the same for both the compartments, this gives 
\[
P_{t}=\left[\begin{array}{ccc}
1-\varrho & 0 & 0\\
0 & 1-\varrho & 0\\
\varrho & \varrho & 1
\end{array}\right],\qquad B_{t}=\left[\begin{array}{ccc}
\beta\phi^{2}(1-\nu) & \,\beta\phi f(\phi)\nu\, & 0\\
\beta\phi f(\phi)(1-\nu) & \,\beta f(\phi)^{2}\nu\, & 0\\
0 & 0 & 1
\end{array}\right]
\]
where $\varrho$ is a removal rate (perhaps consisting of an artificial
and a natural part) and $\beta$ is an unknown constant. This gives
\[
\barT_{t}=\beta C+(1-\varrho)E,\qquad C=\left[\begin{array}{cc}
\phi^{2}(1-\nu) & \,\phi f(\phi)\nu\\
\phi f(\phi)(1-\nu) & \,f(\phi)^{2}\nu
\end{array}\right]
\]
with 
\[
\varrho(\barT_{t})=\beta\rho(C)+(1-\varrho).
\]
As the characteristic polynomial of $C$ is 
\[
\lambda^{2}-\lambda g,\qquad g=g(\phi,\nu)=\phi^{2}(1-\nu)+f(\phi)^{2}\nu
\]
we clearly have $\rho(C)=g.$ 

Now say that our goal is to decrease $\rho(U_{t})$ to a value $r$
by finding appropriate $\phi=\phi(\nu)$, for any $\nu.$ In order
to do so, we have to solve
\[
\beta g(\phi(\nu),\nu)+(1-\varrho)=r.
\]
Clearly, $\phi(0)=\phi_{0}\defined\sqrt{\frac{r-1+\varrho}{\beta}}$.
For $\nu>0$ we get, by the Implicit function theorem, 
\[
\frac{\partial}{\partial\nu}\phi=\frac{f(\phi(\nu))^{2}-\phi(\nu)^{2}}{2\phi(\nu)(1-\nu)+2f(\phi(\nu))f'(\phi(\nu))\nu}.
\]
Note that the derivative depends neither on $r$ nor on $\varrho$
. Thus we can easily compute how the non-compliance influences strictness
of the necessary restrictions. For instance, we can get by the first-order
Taylor expansion, that, 
\[
\phi(\nu)\doteq\phi_{0}+\nu\frac{f(\phi_{0})^{2}-\phi_{0}^{2}}{2\phi_{0}}=\phi_{0}\left(1-\frac{\nu}{2}\right)+\nu\frac{f(\phi_{0})^{2}}{2\phi_{0}}
\]
for $\nu$ close to zero. 
\end{example}


\section{Estimation}

For any stochastic process $A$ and integers $s>t$, denote $\hat{A}_{s|t}=\E(A_{s}|\G_{t})$.
When $T_{\tau}\in\G_{t},t<\tau\leq s-1$ (which is trivially true
if $s=t+1$), we get that

\begin{multline*}
\left[{\hat{X}_{s|t}\atop \hat{Y}_{s|t}}\right]=\E\left(\left.\E\left(\left.\left[{\hat{X}_{s}\atop \hat{Y}_{s}}\right]\right|\F_{s-1}\right)\right|\G_{t}\right)=\left[{E\atop F}\right]\left(T_{s-1}\hat{X}_{s-1|t}+\hat{I}_{s-1|t}\right)\\
=\left[{E\atop F}\right]\left(T_{t,s-1}X_{t}+\sum_{\theta=t}^{s-1}T_{\theta+1,s-1}\hat{I}_{\theta|t}\right),
\end{multline*}
\begin{multline*}
W_{s|t}\defined\var\left(\left.X_{s}\right|\G_{t}\right)=\var\left(\left.\E\left(\left.X_{s}\right|\F_{s-1}\right)\right|\G_{t}\right)+\E\left(\left.\var\left(\left.X_{s}\right|\F_{s-1}\right)\right|\G_{t}\right)\\
=\var\left(T_{s-1}X_{s-1}+I_{s-1}|\G_{t}\right)+\E\left(\left.\Lambda_{s-1}(X_{s-1})\right|\G_{t}\right)\\
=T_{s-1}W_{s-1|t}T_{s-1}^{T}+2T_{s-1}\cov(X_{s-1},I_{s-1}|\G_{t})+\var\left(I_{s-1}|\G_{t}\right)+\Lambda_{s-1}(\hat{X}_{s-1|t})
\end{multline*}
and
\[
V_{s|t}\defined\var\left(\left.{X_{s}\atop Y_{s}}\right|\G_{t}\right)=\var\left(\left.{X_{s}\atop FX_{s}+\epsilon_{s}}\right|\G_{t}\right)=\left[{E\atop F}\right]W_{s|t}\left[{E\atop F}\right]^{T}+\mathrm{diag\left({0_{k}\atop \Gamma_{s-1}\hat{X}_{s-1|t}}\right)}
\]
Unfortunately, due to the non-Gaussianity, we do have analytical formulas
neither for $X_{t|t}$ nor for $\mathrm{W_{t|t}}$, so we can formulate
neither the likelihood function nor a least square estimate. Two,
from the computational point of view equivalent, ways to cope with
this could either using estimates of the conditional expectation and
variance, or normally approximating the residuals. We go the latter
way: in the present Section, we assume that $\left.\left[{X_{t+1}\atop Y_{t+1}}\right]\right|\F_{t}$
is normal with mean given by (\ref{eq:moments}) and 
\[
\var\left(\left.{X_{t+1}\atop Y_{t+1}}\right|\F_{t}\right)=\left[{E\atop F}\right]\Lambda_{t}(X_{t}\vee0)\left[{E\atop F}\right]^{T}+\mathrm{diag\left({0_{k}\atop \Gamma_{t}(X_{t}\vee0)}\right)}
\]
 Given this assumption we have, by Eatonxxx, 
\begin{multline*}
\hat{X}_{t|t}=I_{t-1}+\hat{X}_{t|t-1}+L_{t}\left(Y_{t}-\hat{Y}_{t|t-1}\right)\\
L_{t}\defined V_{t|t-1}^{XY}(V_{t|-1}^{YY})^{-1}=W_{t|t-1}F^{T}D_{t}^{-1},\qquad D_{t}=FW_{t|t-1}F^{T}+\mathrm{diag}(\Gamma_{t}\hat{X}_{t|t-1})
\end{multline*}

\begin{multline*}
W_{t|t}=V_{t|t-1}^{XX}-V_{t|t-1}^{XY}(V_{t|t-1}^{YY})^{-1}V_{t|t-1}^{YX}=W_{t|t-1}-W_{t|t-1}F^{T}D_{t}^{-1}FW_{t|t-1}
\end{multline*}
Note that $L_{t}$ may be seen as a conditional version of a Kalman
gain matrix.

As for the estimation itself, assume that $F=F(\Theta_{0}),$$P_{t}=P_{t}(\Theta_{0})$,
$G_{t}=G_{t}(\Theta_{0})$, $\Gamma_{t}=\Gamma_{t}(\Theta_{0})$ and
$I_{t}=I_{t}(\Theta_{0})$ where $\Theta_{0}\in\R^{r}$ is an unknown
parameter. For its estimation, it is possible to to use either nonlinear
least squares, i.e. 
\[
\hat{\Theta}=\arg\min\sum_{t}(Y_{t}-\hat{Y}_{t|t}(\Theta))^{T}D(\Theta)_{t}^{-1}(Y_{t}-\hat{Y}_{t|t}(\Theta))
\]
or maximum likelihood 
\[
\tilde{\Theta}=\arg\min\sum_{t}\varphi((Y_{t}-\hat{Y}_{t|t}(\Theta)),D_{t}(\Theta)),\qquad\varphi(x,v)=-\frac{k\ln2\pi+\ln\mathrm{det}(v)+x^{T}v^{-1}x}{2}
\]
Both these estimators are consistent and asymptotically normal under
some conditions, see $G$acob, $G$acob respectively (footnote one
dim), verifying these conditions for our model is, however, beyond
the scope of this short research letter and remains as topic of a
future research. Mote also, that our proof of Proposition \ref{prop:as}
is not valid for the approximate model, as $\barX$ is not necessarily
positive here

\section{Application to The COVID Pandemics in Czech Republic}

We applied our model to the data from the first and second wave of
the pandemics in the Czech Republic, starting from Feb 24, 2020. We
considered a genralized of our model from Example \ref{exa:e1}. The
compartments of the general model are shown in the following Picture:

\includegraphics[scale=0.6]{big}

To the compartments from Example \ref{exa:e1} we added their ``detected''
versions, distinguishing detection when being asymptomatic (subscript
$\delta$) and when being symptomatic (subscript $d$). Moreover,
for both the symptomatic and the asymptomatic course, we addde the
states in which the individuals are RNA positive, but not infectious
(denoted by $J$) and we distinguish two ``removed'' states: recovered
($R$) and dead ($D$). All the $J$ and $R$ states have three versions:
undetected, detected when asymptomatic and detected when symptomatic.
Assuming that once the undetected course ends by death, the detection
takes place before the time of death, we have only two death states:
detected when asymptomatic and detected when symptomatic. In the following
Table we list the transition parameters of the model and the way of
their evaluation.

Similarly as in the Example, we assumed equal infectiousness $\beta_{t}$
for all the infectious states $I_{a},I_{p},I_{s}$, varying in time:
\begin{equation}
\gamma_{t}=\beta c_{t}p_{t}\label{eq:gamma}
\end{equation}
where $\beta$ is an (estimated) constant, $c_{t}$ is the contact
reduction (with $c_{0}=1$) and $p_{t}$ is the reduction caused by
personal protection. 

The ``removal'' are $\eta$ and $\theta$ are also time-varying.
Reflecting the weekly pattern of reporting, we assume 
\[
\theta_{t}=\phi_{t+1}\theta,\qquad\eta_{t}=\phi_{t+1}\eta
\]
where $\phi_{t}$ is the adjustment for the day of the week, having
different values for different weekdays, being computed by a standard
way from $\ln R_{t}$ where $R_{t}$ is the total number of positive
cases. \footnote{In particular we assume $\Delta R_{t}=\phi_{t}\Delta r_{t}$ where
$r$ is a ``trend'' without oscillations and $\prod_{i=1}^{7}\phi_{i}=1$,
which gives $\ln(\Delta R_{t})=\ln(\phi_{t})+\ln(\Delta r_{t})$ and
$\sum_{i=1}^{7}\ln\phi_{i}=0$. Assuming $r$ to be locally linear,
we estimate $\ln\phi_{t}=\frac{1}{4}\sum_{i=1}^{4}\left(\ln(\Delta R_{t-7i})-\frac{1}{7}\sum_{j=-3}^{3}\ln(\Delta R_{t-7i+j})\right)-\frac{1}{18}\sum_{i=1}^{28}\ln(\Delta R_{t-i})$ } 

The following graph sillustrated de-seasoning we made:

\includegraphics[scale=0.5]{dayadjust}

We use three data series as observations: the daily numbers of detected
cases, distinguished between symptomatic (S) asymptomatic ($A$) and
daily numbers of dead ($D)$, i.e.
\[
Y_{t}=\left[\begin{array}{c}
A_{t}\\
S_{t}\\
D_{t}
\end{array}\right].
\]
As for the transformation matrix $F$, we put $F=(f^{ij})$ where
$f^{1i}$is one/zero if the state $i$ is/is not \emph{detected asymptomatic
on }see Figure, and similarly with $f^{2i}$ -- \emph{detected symptomatic}
and $f^{3i}$ -- \emph{dead}. 

Assuming the imports only to the state $E$, we take 
\[
I_{t}=r_{t}M_{t+8}
\]
where $M_{t}$ is the number of detected with the indicated infection
abroad and $r_{t}$ is a multiplication factor. We took $r_{t}=\iota$
for $t<30$ (the first month), where $\iota$ is an unknown parameter;
this allowed us to reflect the excess numbers of dead in comparison
with the detected, which suggests that many of the cases remained
unnoticed in the beginning of the pandemics. For the next months of
the pandemics, we took $r_{t}=\frac{1}{1-\alpha}$ to reflect the
fact that fraction $\alpha$ of imports will remain unnoticed.

The following plots show the mentioned time series

\begin{tabular}{|c|c|}
\hline 
\includegraphics[scale=0.3]{A} & \includegraphics[scale=0.3]{S}\tabularnewline
\hline 
\hline 
\includegraphics[scale=0.3]{D} & \includegraphics[scale=0.3]{M}\tabularnewline
\hline 
\end{tabular}

To compute $c_{t}$ and $p_{t}$ we used {[}Dan{]} which is a longitudinal
study, asking a panel of 3000 respondents about their (weekly) intense
contacts, observance of several personal protection measures (see
Table xx), and some other variables. Using this, we compute $c_{t}=\frac{C_{t-\Delta}}{C_{0}}$
where $C_{t}$is the average reported number of contacts people had
at a given time (the weekly reponses values are linearly interpolated
assuming the responded values on Wednesdays) and $\Delta$ is an unknown
parameter, reflecting our lack of knowledge concerning a delay. 

With determining the level of personal protection $p_{t}$, the situation
is more complicated, as the study monitors observance of several protective
measures. However, if we assume that the $i$-th measure reduces the
probability of infection by $\lambda_{i}$, we get that, denoting
$\pi_{t}^{i}$ the average observance of the measure among the respondents,
that the average reduction brought by the the measure will be $(1-\pi_{t}^{i})\times1+\pi_{t}^{i}(1-\lambda^{i})=1-\pi_{t}^{i}\lambda^{i}.$
Consequently, the total reduction will be
\[
p_{t}=\prod_{i=1}^{q}(1-\pi_{t}^{i}\lambda^{i}).
\]
where $q$ is the number of measures. Unfortunately, $\lambda_{i}$
are unknown and their estimation would bring a serious danger of over-fitting
and/or co-linearity (series $\pi_{t}^{1},\dots,\pi_{t}^{q}$ are almost
perfectly co-related). To overcome this difficulty, we applied factor
analysis to $(C_{t},\pi_{t}^{1},\dots,\pi_{t}^{q})$ on the respondent
level, treating the responses in different times as separate observations.
As a result, we extracted two main factors, the first strongly related
to contact reduction, the second lacking connection with $C$, as
shown in the following table 

xxx

It can be seen that, while the first factor speaks more about contacts,
the second one concerns personal protection. Thus we approximate 
\[
\pi_{t}^{i}\doteq\overline{\pi}_{t}^{i}+\nu_{t}f_{t}
\]
where $\overline{\pi}_{t}^{i}$ is the average of $\pi_{t}^{i}$ over
time and respondents, $\nu_{t}$ is a constant and $f_{t}$ is average
of the second factor over respondents at $t$. Having that. we could
approximate 
\begin{multline*}
p_{t}\doteq\prod_{i=1}^{q}(1-\lambda^{i}(\overline{\pi}_{t}^{i}+\nu_{t}f_{t}))=\exp\left\{ \sum_{i=1}^{q}\ln(1-\lambda^{i}(\overline{\pi}_{t}^{i}+\nu_{t}f_{t}))\right\} \\
\doteq\exp\left\{ -\sum_{i=1}^{q}\lambda^{i}(\overline{\pi}_{t}^{i}+\nu_{t}f_{t})\right\} =\omega_{0}\exp\left\{ -\omega_{1}f_{t}\right\} 
\end{multline*}
where $\omega_{0},\omega_{1}\geq0$. Consequently, we take 
\[
\beta_{t}=bc_{t}\exp\left\{ -\omega_{1}f_{t}\right\} ,\qquad b=\beta\omega_{0}.
\]

Finally, not assuming additional errors of observations, we $\Gamma_{t}\equiv0$.

For estimation, we used Weighted least squares with weights $w_{t}=\frac{1}{\max(\overline{r}_{t},20)}$
where $\overline{r}_{t}$ is the weekly average of reported positive
over past week.

The results of the estimation, based on data up to November 5th, 2020,
can be seen in the following Table

\begin{tabular}{lcccc}									 	&	Estimate	&	Std. Error	&	z	&	Significance	\\ \hline
$\iota$	&	$1.82882$	&	$0.204$	&	$8.953$	&	$0^{***}$	\\ $b_{0}$	&	$1.19497$	&	$0.302$	&	$3.956$	&	$0.0001^{***}$	\\ $\omega_{1}$	&	$2.30022$	&	$1.004$	&	$2.291$	&	$0.022^{*}$	\\ $\omega_{1}$	&	$5.14679$	&	$0.314$	&	$16.39$	&	$0^{***}$	\\ $\Delta$	&	$0.00198165$	&	$0$	&	$5.722$	&	$0^{***}$	\\ $\theta$	&	$0.0347199$	&	$0$	&	$257.228$	&	$0^{***}$	\\ $\eta$	&	$0.0347199$	&	$0$	&	$142.484$	&	$0^{***}$	\\ \hline  \end{tabular}									 

Yet the standard errors and significance are only indicative here
(we did note verify regularity conditions), the result suggest a good
fit 

The following graphs shows standardized residuals and standardized
mean absolute errors of a one day ahead prediction and the predictions
of weekly amounts. The red lines are always predictions and the blue
ones actual data.

\begin{tabular}{|c|c|c|c|}
\hline 
 & Standardized residuals & Stamdardized MAE & Seven-day prediction\tabularnewline
\hline 
\hline 
$A$ & \includegraphics[scale=0.4]{ar} & \includegraphics[scale=0.35]{a1} & \includegraphics[scale=0.25]{a7}\tabularnewline
\hline 
$S$ & \includegraphics[scale=0.4]{sr} & \includegraphics[scale=0.35]{s1} & \includegraphics[scale=0.25]{s7}\tabularnewline
\hline 
$D$ & \includegraphics[scale=0.4]{dr} & \includegraphics[scale=0.35]{d1} & \includegraphics[scale=0.25]{d7}\tabularnewline
\hline 
\end{tabular}

The residuals are standardized in the sense that the actual residual
is divided by the standard deviation of the one-day ahead prediction
(its square is found on the diagonal of the $V_{t\text{|}t-1})$.
The MAEs are standardized divided by the weekly averages, so they
show the MAE of a relative increments. The predictions are in-sample
in the sense that they use parameters originated from the estimation
over the whole period; however, the predictions are always based only
on the data available at the moment of prediction.

These results clearly show limitations of the present simple parametrization
of the model. First of all, the standardized residuals are far from
having unit variance which means that the actual variance of the observations
is much bigger than that predicted by the model, the problem being
more severe with the reported infections and less severe with deaths.
It is clear that assuming a non-zero $\Gamma_{t}$ (i.e. observation
errors), can make the variances right; then, however, the prediction
errors increase because the model starts to give less weight to the
observations in comparison with its state.

It is also clear from the graphs that, the model tends to over-estlimate
reported infections between the waves which can be naturally attributed
to better tracing when numbers are low. In addition, asymptomatic
numbers are over-estimated even during the first wave. A natural solution
here is to assume a variable rates $\theta$, which was proved to
increase the prediction power,\footnote{In the Situation Report by BISOP {[}cite{]}, for instance, \$\textbackslash theta\_t\$
was approximated by a piece-wise linear function with kinks at 24/02/20,
16/03/20, 26/04/20. 04/07/20, 06/08/20, 16/09/20 and 27/10/20 and
taking values $\theta_{0},\dots,\theta_{6}$ at these points. We took
$\mu=0.0013132$, and we got the following parameters by ML estimation:

\begin{tabular}{lcccc}									 	&	Estimate	&	Std. Error	&	z	&	Significance	\\ \hline $\iota$	&	$9.96795$	&	$0.801$	&	$12.441$	&	$0^{***}$	\\ $b$	&	$0.618509$	&	$0.035$	&	$17.539$	&	$0^{***}$	\\ $\alpha$	&	$0.549047$	&	$0.026$	&	$21.402$	&	$0^{***}$	\\ $\omega_{1}$	&	$0.550946$	&	$0.061$	&	$8.986$	&	$0^{***}$	\\ $\theta_0$	&	$0.0194497$	&	$0.015$	&	$1.317$	&	$0.0939^{*}$	\\ $\theta_1$	&	$0.00402584$	&	$0.003$	&	$1.318$	&	$0.0937^{*}$	\\ $\theta_2$	&	$0.0204434$	&	$0.006$	&	$3.346$	&	$0.0004^{***}$	\\ $\theta_3$	&	$0.0260418$	&	$0.004$	&	$5.789$	&	$0^{***}$	\\ $\theta_4$	&	$0.0248783$	&	$0.004$	&	$6.784$	&	$0^{***}$	\\ $\theta_5$	&	$0.00966628$	&	$0.001$	&	$16.701$	&	$0^{***}$	\\ $\theta_6$	&	$0.0221601$	&	$0.001$	&	$14.897$	&	$0^{***}$	\\ $\eta$	&	$0.624971$	&	$0.007$	&	$89.31$	&	$0^{***}$	\\ \hline \end{tabular}									 } but it can bring a danger of over-fitting.

\end{document}

\bibliographystyle{plain}
\bibliography{/home/martin/Documents/s/smid}

\begin{itemize}
\item Estimating parameters in stochastic compartmental models using Markov
chain methods GAVIN J GIBSON (continuous Markov chain, Bayesian inference) 
\end{itemize}
\end{document}

\section*{Second order $p_{t}$}

\begin{multline*}
p_{t}\doteq\prod_{i=1}^{q}(1-\lambda^{i}(\overline{\pi}_{t}^{i}+\nu_{t}f_{t}))=\exp\left\{ \sum_{i=1}^{q}\ln(1-\lambda^{i}(\overline{\pi}_{t}^{i}+\nu_{t}f_{t}))\right\} \\
\doteq\exp\left\{ \sum_{i=1}^{q}\left[-\lambda^{i}(\overline{\pi}_{t}^{i}+\nu_{t}f_{t})-\frac{(\lambda^{i}(\overline{\pi}_{t}^{i}+\nu_{t}f_{t}))^{2}}{2}\right]\right\} =\omega_{0}\exp\left\{ -\omega_{1}f_{t}-\omega_{2}f_{t}^{2}\right\} 
\end{multline*}
where $\omega_{0},\omega_{1},\omega_{2}\geq0$. Consequently, we take
\[
\gamma_{t}=\beta_{0}c_{t}\exp\left\{ -\omega_{1}f_{t}-\omega_{2}f_{t}^{2}\right\} ,\qquad\beta_{0}=\beta\omega_{0}.
\]


\subsection*{Linearized model}

except for $ $ and $\omega_{1}$, which are (locally) colinear --
the correlation of the corresponding estimators is $0.99$. In order
to disentangle these parameters, we re-estimated the model with the
linear approximation
\[
\gamma_{t}=c_{t}(b-\varpi_{1}f_{t})
\]
results of which are in the following Table.

\begin{tabular}{lcccc}
&	Estimate	&	Std. Error	&	z	&	Significance	\\ \hline									 $\iota$	&	$9.86793$	&	$0.464$	&	$21.267$	&	$0^{***}$	\\ $\Delta$	&	$4.7787$	&	$0.657$	&	$7.272$	&	$0^{***}$	\\ $b_0$	&	$0.420202$	&	$0.041$	&	$10.203$	&	$0^{***}$	\\ $\omega_{1}$	&	$0.12406$	&	$0.084$	&	$1.475$	&	$0.0701^{*}$	\\ $\theta$	&	$0.0339621$	&	$0$	&	$273.996$	&	$0^{***}$	\\ $\eta$	&	$0.623455$	&	$0.004$	&	$148.797$	&	$0^{***}$	\\ \hline  \end{tabular}									 

This clearly shows, that, while contacts are indisputable driver of
the epidemics, this (simple) model confirms the significance of personal
protection only on \$10\$ per cent level; this, however, is most likely
caused by small variation of $f_{1}$ over time. The following graphs
show course of $c_{t},$estimated course of $\beta_{t}$ for both
the original and the linear model, and the estimated course of $\text{\ensuremath{\beta}. }$

\begin{tabular}{|c|c|}
\hline 
\includegraphics[scale=0.3]{cp} & \includegraphics[scale=0.3]{gamma}\tabularnewline
\hline 
\end{tabular}

It is clear that the original and the linearized models are practically
equivalent, so we will refer only to the linear one until the end
of the paragraph.

x

x

x

y

A question arises what $I_{t}$ and $P_{t}$ should be so that $Z_{t}\rightarrow0$. 

Let $U=\{E,I_{a},I_{s},I_{n}\}$ We have 
\[
\barP=\left[\begin{array}{cccc}
p^{E} & \iota & \iota & \iota\\
p^{Ea} & p^{aa} & 0 & 0\\
0 & p^{as} & p^{ss} & 0\\
p^{En} & 0 & 0 & p^{nn}
\end{array}\right]
\]
and we have 
\begin{multline*}
|\barP-\lambda I|=\left|\begin{array}{cccc}
p^{E}-\lambda & \iota & \iota & \iota\\
p^{Ea} & p^{aa}-\lambda & 0 & 0\\
0 & p^{as} & p^{ss}-\lambda & 0\\
p^{En} & 0 & 0 & p^{nn}-\lambda
\end{array}\right|\\
=(p^{E}-\lambda)\left|\begin{array}{ccc}
p^{aa}-\lambda & 0 & 0\\
p^{as} & p^{ss}-\lambda & 0\\
0 & 0 & p^{nn}-\lambda
\end{array}\right|-\iota\left|\begin{array}{ccc}
p^{Ea} & 0 & 0\\
0 & p^{ss}-\lambda & 0\\
p^{En} & 0 & p^{nn}-\lambda
\end{array}\right|\\
+\iota\left|\begin{array}{ccc}
p^{Ea} & p^{aa}-\lambda & 0\\
0 & p^{as} & 0\\
p^{En} & 0 & p^{nn}-\lambda
\end{array}\right|-\iota\left|\begin{array}{ccc}
p^{Ea} & p^{aa}-\lambda & 0\\
0 & p^{as} & p^{ss}-\lambda\\
p^{En} & 0 & 0
\end{array}\right|\\
=(p^{E}-\lambda)(p^{nn}-\lambda)(p^{aa}-\lambda)(p^{ss}-\lambda)\\
-\gamma(p^{nn}-\lambda)(p^{ss}-\lambda)+\iota(p^{nn}-\lambda)p^{Ea}p^{as}-\gamma(p^{ss}-\lambda)(p^{aa}-\lambda)
\end{multline*}
\begin{align*}
\frac{\partial f}{\partial\iota} & =-p^{Ea}(p^{nn}-\lambda)(p^{ss}-\lambda)+(p^{nn}-\lambda)p^{Ea}p^{as}-p^{En}(p^{ss}-\lambda)(p^{aa}-\lambda)
\end{align*}
\[
\frac{\partial f}{\partial p^{ss}}=(p^{E}-\lambda)(p^{nn}-\lambda)(p^{aa}-\lambda)-\gamma(p^{nn}-\lambda)-\gamma(p^{aa}-\lambda)
\]
\\
\begin{multline*}
p^{ss}'(\iota)=\frac{f_{\iota}}{f_{ss}}
\end{multline*}

\end{document}

x

x

x

x

x

x

x

x

x

x

x

x

x

Let $Y_{t}\in\mathbb{N}^{k+1}$ be the state vector. For $s\neq E$,
$Y_{t}^{s}|Y_{t-1}=\sum_{j}\Bi(Y_{t-1}^{j}M_{t}^{s,j})$ and\textasciiacute{}
$\epsilon_{t}^{E}|Y_{t-1}=\Bi(Y_{t-1}^{E},P_{t}^{E,E})+\Bi(N,\sum_{s\neq E}\frac{Y_{t-1}^{s}}{N}\iota_{t}^{s})\doteq\dots+\mathrm{Po}(\sum_{s\neq E}Y_{t-1}^{s}\iota_{t}^{s})$
i.e.
\[
Y_{t}=M_{t}Y_{t-1}+\epsilon_{t}
\]
 where
\[
\var(\epsilon_{t}^{E}|Y_{t-1})=Y_{t-1}^{E}M_{t}^{E,E}(1-M_{t}^{E,E})+\sum_{j\neq E}M_{t}^{s,j}Y_{t-1}^{j},\qquad\var(\epsilon_{t}^{s}|Y_{t-1})=\sum_{j}M_{t}^{s,j}(1-M_{t}^{s,j})Y_{t-1}^{j}
\]
Further, 
\[
\Delta R_{t}|Y_{t-1}^{j}\sim\sum_{j\leq k+1}\Bi(Y_{t-1}^{j},P_{t}^{j,R})
\]
Nearly clearly, 
\[
\Delta R_{t}=\tau(t)+\mathcal{E}_{t},\qquad\tau(t)=N_{t}M_{t-1}\dots M_{1}Y_{0},\qquad N_{t}=(P_{t}^{E,R},\dots,P_{t}^{S_{k},R})
\]
where, $\mathcal{\E E}_{t}=0$ and hopefully, $\var(\mathcal{E}_{t}|\Delta R_{1},\dots,\Delta R_{t-1})$
is proportional to $Y_{t-1}^{R}$ (maybe can be computed when residuals
are approximated by normal ones).

The contagion probability is assumed to fulfill 
\[
\iota_{t}=\beta c_{t}(1-am_{t})
\]
where $\beta,\mu$ are parameters constant, $c_{t}$ is the contact
restriction and $m_{t}$ is the rate of self-protecting activities. 

To stop epidemic growing, we need 
\[
\|T_{t}\|\leq1
\]
we can determine set of $c$ and $m$'s fulfilling this.
\begin{proof}
Further we have 

\begin{multline*}
\var(X_{t})=\E(\var(X_{t}|\G_{0}))+\var(\E(X_{t}|\G_{0}))\\
=\E(\underbrace{\var\left(M_{t-1}+T_{t-1}X_{t-1}|\G_{0}\right)}_{\preceq\var(M_{t-1}+T_{t-1}X_{t-1})})+\underbrace{\E\Lambda(\E(X_{t-1}|\G_{0}))}_{=\Lambda_{t}(\E(X_{t-1}))}+\var(M_{t-1}+T_{t-1}X_{t-1})\\
\preceq2\var(M_{t-1}+T_{t-1}X_{t-1})+\Lambda(\E(X_{t-1}))\\
\preceq4\var(M_{t-1})+4\var(T_{t-1}X_{t-1})+\Lambda(\E(X_{t-1}))\\
\preceq4\var(M_{t-1})+4T_{t-1}\var(X_{t-1})T_{t-1}^{T}+k\|T\|\|x\|\E(X_{t-1}))
\end{multline*}
as
\[
\|\Lambda_{t}(x)\|_{1}\leq\mathrm{\|\sum_{k}diag}(\iota_{t}^{i}??+p_{t}^{i})x^{i}\|_{t}\leq k\|T\|\|x\|
\]
(laer, we have $\Lambda_{t}(\E(X_{t-1}))\rightarrow0$.)
\end{proof}

\subsection*{Variance}
\begin{proof}
By the parallerogram inequalitiex (Luke¨ rovn. pravidlo, Clarkson),
we have 
\[
\var(M_{t-1}+T_{t-1}X_{t-1})\leq2\var(M_{t-1})+2\var(T_{t-1}X_{t-1})=
\]
we have 
\[
\var(T_{t-1}X_{t-1})=T_{t-1}\var(X_{t-1})T_{t-1}^{T}
\]
\end{proof}

\section*{Old}

Compartments 
\begin{description}
\item [{$S$}] - suspectible (constant for simplicity)
\item [{$E$}] - exposed noninfectious
\item [{$A$}] - asymptotic
\item [{$I$}] - symptomatic
\item [{$P$}] - reported
\item [{$R$}] - removed
\end{description}
Transition Matrix $ $

\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
 & S & E & A & I & P & R &  & \tabularnewline
\hline 
\hline 
\multirow{1}{*}{S} & $1-\dots$ & $\kappa_{t}(\epsilon_{A}A+\text{\ensuremath{\epsilon_{I}}}I)$ &  &  &  &  &  & \tabularnewline
\hline 
E &  & $1-\dots$ & $\alpha$ &  &  &  &  & \tabularnewline
\hline 
A &  &  & $1-\dots$ & $\iota$ & $\pi_{A}$ & $\rho_{A}$ &  & \tabularnewline
\hline 
I &  &  &  & $1-\dots$ & $\pi_{I}$  & $\rho_{I}$ &  & \tabularnewline
\hline 
P &  &  &  &  & 1 &  &  & \tabularnewline
\hline 
R &  &  &  &  &  & 1 &  & \tabularnewline
\hline 
 &  &  &  &  &  &  &  & \tabularnewline
\hline 
\end{tabular}

Beginning of epidemic -$|S|$ very large

Equations
\[
E_{t}=(1-\alpha)E_{t-1}+\epsilon_{A}A_{t-1}+\epsilon_{I}I_{t-1}-\alpha+\eta_{t,E}
\]
\begin{align*}
A_{t} & =(1-\iota-\pi_{A}-\rho_{A})A_{t-1}+\alpha E_{t-1}+\eta_{t,A}
\end{align*}
\begin{align*}
I_{t} & =(\pi_{A}+\rho_{A})I_{t-1}+\gamma+\eta_{t,I}
\end{align*}
\[
\Delta P_{t}=\pi_{A}A_{t-1}+\pi_{I}I_{t-1}+\eta_{t,P}
\]

$\eta_{t,E}|\F_{t-1}\dot{\sim}\text{\ensuremath{\mathcal{N}(0,(1-\alpha)E_{t-1}+\epsilon_{A}A_{t-1}+\epsilon_{I}I_{t-1}),}etc.}$

Up to observed (removed may be modeled separately):
\[
\left[\begin{array}{c}
E\\
A\\
I\\
P
\end{array}\right]=\underbrace{\left[\begin{array}{cccc}
\omega_{E} & \iota_{t}\epsilon_{A} & \iota_{t}\epsilon_{I} & 0\\
\alpha & \omega_{A} & 0 & 0\\
0 & \iota & \omega_{I} & 0\\
0 & \pi_{A} & \pi_{I} & 0
\end{array}\right]}_{M_{t}}\left[\begin{array}{c}
E\\
A\\
I\\
P
\end{array}\right]+\eta_{t}
\]

\[
P_{t}=(0,0,0,1)M_{t}M_{t-1}....M_{0}(\alpha,0,0,0)+\mathcal{E}_{t}
\]

where $\alpha$ is an initial seed. Can be rewritten as 
\[
\Delta P_{t}=\tau(t)\alpha+\mathcal{E}_{t}
\]

In particular

\[
\tau(0)=a
\]

The system can be estimated by MLE\end{document}

x

x

x

x

x

x

x

x

x

x

x

x

We have 

$A$ unobserved.

We have
\[
A_{t}=\underbrace{(1+\mu_{t}-\gamma_{A}-\iota)}_{\theta_{t}}A_{t-1}+\nu I_{t-1}^{k}+\eta_{A,t}
\]

we rewrite 

\begin{multline*}
A_{t}=\theta_{t}A_{t-1}+\nu I_{t-1}^{k}+\eta_{A,t}=\theta_{t}(\theta_{t-1}A_{t-2}+\nu I_{t-2}^{k}+\eta_{A,t-1})+\nu I_{t-1}^{k}+\eta_{A,t}\\
=Z_{t}+\epsilon_{t},\\
Z_{t}=(\theta_{1}\times\dots\times\theta_{t})\alpha_{0}+\nu\sum_{\tau=1}^{t-1}(\theta_{\tau+2}\times\dots\times\theta_{t})I_{\tau}^{k}=\theta^{s\wedge t}\theta_{1}^{(t-s)\wedge0}\alpha_{0}+\nu\sum_{\tau=1}^{t-1}\theta^{(s-(\tau+1))\wedge0}\theta_{1}^{t-(s\vee\tau+1)}\\
\epsilon_{t}=\sum_{\tau=1}^{t}(\theta_{\tau+1}\times\dots\times\theta_{t})\eta_{A,t}
\end{multline*}
where $\alpha_{0}=A_{0}$ (to be estimated) $Z$ may be evaluated
recursively (assuming $I_{0}=0$): 
\[
Z_{0}=\alpha_{0},Z_{1}=\theta_{1}\alpha_{0}+\nu I_{0},Z_{2}=\theta_{2}\theta_{1}\alpha_{0}+\theta_{2}\nu I_{0}+\nu I_{1},\dots Z_{t}=\theta_{t}Z_{t-1}+\nu I_{t-1}
\]

This gives 
\[
\Delta I_{t}=\gamma+e_{t},\qquad e_{t}=\iota\epsilon_{t-1}+\eta_{I,t}
\]
It is not easy to compute $\var(e|\G_{t-1})$ (possible in normal
case via well known formula for conditional distribution, no time
here for this). So we dare to estimate the variances by $\var(\bullet_{t}|\F_{\t-1}),$
which equals to those of etas.

Let $s$ be the threshold when different policy started to be applied. 

Summary of parameters

\subsubsection*{%
\begin{tabular}{|c|c|c|c|}
\hline 
 &  &  & \tabularnewline
\hline 
\hline 
$\iota\alpha_{0}$ &  &  & \tabularnewline
\hline 
$\theta$ & $1+\mu-\gamma_{A}-\iota$ & $\theta_{t}=\theta$ for $t<s$ & \tabularnewline
\hline 
$\theta_{1}$ &  & $\theta_{t}=\theta_{1}$ for $t\protect\geq s$ & \tabularnewline
\hline 
$\iota\nu$ &  &  & \tabularnewline
\hline 
 &  &  & \tabularnewline
\hline 
 &  &  & \tabularnewline
\hline 
\end{tabular}
\[
\]
Estimation}

MLE. Loglikelihood
\[
C-\frac{1}{2w}\sum_{i=1}^{T}(Y_{t}-\Delta I_{t})^{2}
\]
\[
Y_{t}=\gamma=\iota\alpha_{0}\theta^{s\vee}\theta_{1}^{(t-1-s)\vee0}+\iota\nu\sum_{\tau=1}^{t-2}\theta^{(s-(\tau+1))\wedge0}\theta_{1}^{t-1-(s\vee\tau+1)}
\]
derivatives by the multipliers clear, by thetas recursively: by $\theta$:
\begin{align*}
Y'_{0} & =0,\qquad Y'_{1}=\begin{cases}
\iota\alpha_{0}(=Y_{\tau-1}) & s\geq1\\
0 & s<1
\end{cases},\qquad
\end{align*}

\[
Y'_{\tau}=(\theta_{\tau}Y_{\tau-1}+\iota\nu I_{\tau-1})'=\begin{cases}
Y_{\tau-1}+\theta Y'_{\tau-1} & \tau\leq s\\
\theta_{1}Y'_{\tau-1} & \tau>s
\end{cases}
\]
and by $\theta_{1}:$
\[
Y'_{0}=\dots=Y'_{s}=0
\]

\[
Y'_{s+1}=Y_{s},\qquad Y'_{\tau}=Y_{\tau-1}+\theta_{1}Y'_{\tau-1},\tau>s+1\text{}
\]


\section*{Apokryfy}

By Pavuo, chart 1: 

\begin{tabular}{|c|c|c|c|}
\hline 
 &  &  & \tabularnewline
\hline 
\hline 
$\mu$ & infection of $A$ per day & $1$ & \tabularnewline
\hline 
$\nu$ & of $I$ per day & 0.2 & \tabularnewline
\hline 
$\iota$,$\gamma_{A}$ & transitions from $A$ & $\frac{\iota}{\iota+\gamma_{a}}=0.7$ $\iota=5^{-1}$ $\rightarrow$$\gamma_{A}=\frac{1}{12}$ & \tabularnewline
\hline 
$\delta,\gamma_{i}$ & from I & $\frac{\delta}{\delta+\gamma_{i}}=\frac{15}{70}$ ,$\delta+\gamma_{i}=\frac{1}{7}$
$\rightarrow$$\delta=0.03,\gamma=\text{\textasciiacute0.11}$ & \tabularnewline
\hline 
$\phi,\gamma_{C}$ & from $C$ & $\frac{\phi}{\phi+\gamma_{C}}=\frac{1}{3}$,$\phi+\gamma_{C}=\frac{1}{5}\rightarrow\phi=\frac{1}{15},\gamma_{C}=\frac{2}{15}$ & \tabularnewline
\hline 
 &  &  & \tabularnewline
\hline 
\end{tabular}

$\left[\begin{array}{cccc}
p^{s,E}(1-p^{s,E}) & p^{s,I_{1}}p^{s,E}\\
+p^{s,E}p^{s,I_{1}} & p^{s,I_{1}}(1-p^{s,I_{1}}\\
\\
 &  &  & p^{s,R}(1-p^{s,R})
\end{array}\right]$

$p^{s,E}=0,s\neq E$ and let $p^{R,R}=1$ ($R$ is terminal). Then,
assuming CI, we have 

\section*{Apokryfy}

Denote $M_{t}=\E(X_{t},Y_{t}|\G_{t-1})$ and $V_{t}=\var(X_{t},Y_{t}|\G_{t-1})$.
As, given normal approximation,we get
\begin{multline*}
\E\left[\left.{X_{t+1}\atop Y_{t+1}}\right|\G_{t}\right]=\E\left[\E\left[\left.{X_{t+1}\atop Y_{t+1}}\right|\F_{t}\right]\G_{t}\right]\\
=\E\left[\left.\left[{T_{t}\atop FT_{t}}\right]X_{t}\right|\G_{t}\right]=\left[{T_{t}\atop FT_{t}}\right]N_{t},\qquad N_{t}=\E(X_{t}|\G_{t})
\end{multline*}
and 
\begin{multline*}
V_{t+1}=\var\left(\left.\E\left[\left.{X_{t+1}\atop Y_{t+1}}\right|\F_{t}\right]\right|\G_{t}\right)+\E\left(\left.\var\left[\left.{X_{t+1}\atop Y_{t+1}}\right|\F_{t}\right]\right|\G_{t}\right)\\
=\var\left[\left.\left[{T_{t}\atop FT_{t}}\right]X_{t}\right|\G_{t}\right]+\E\left[\left.\begin{array}{cc}
\Lambda_{t} & \Lambda_{t}F^{T}\\
F\Lambda_{t} & F\Lambda_{t}F^{T}+FX_{t}
\end{array}\right|\G_{t}\right]\\
=\left[{T_{t}\atop FT_{t}}\right]W_{t}\left[{T_{t}\atop FT_{t}}\right]^{T}+\sum_{i+1}^{k}\left[\begin{array}{cc}
\Phi_{i} & \Phi_{i}F^{T}\\
F\Phi_{i} & F\Phi_{i}F^{T}
\end{array}\right]N_{t}^{i}+\left[\begin{array}{cc}
J_{t} & J_{t}F^{T}\\
FJ_{t} & FJ_{t}F^{T}+\Gamma tbdN_{t}
\end{array}\right]
\end{multline*}
where $J_{t}=\mathrm{diag}(I_{t}N_{t})$
\end{document}
